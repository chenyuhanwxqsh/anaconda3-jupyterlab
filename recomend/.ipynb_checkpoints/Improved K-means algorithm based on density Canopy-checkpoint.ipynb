{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "from sklearn.metrics import jaccard_score # scikit 0.21\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码中定义了基本的Canopy聚类实现，函数的循环过程为：从canopy_points中取出一点，为这个点创建一个新的簇，并添加所有距离小于T2的点到这个簇中，并且从canopy_points中移除所有距离小于T1的点（因为这些点不应该被包括在其他簇中），直到canopy_points为空。最后返回包含所有簇信息的字典canopies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X shoudl be a numpy matrix, very likely sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 = Distance to centroid point to not include in other clusters\n",
    "# T2 = Distance to centroid point to include in cluster\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 < T2 will have points which reside in no clusters\n",
    "# T1 == T2 will cause all points to reside in mutually exclusive clusters\n",
    "# Distance metric can be any from here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "# filemap may be a list of point names in their order in X. If included, row numbers from X will be replaced with names from filemap. \n",
    " \n",
    "def canopy(X, T1, T2, distance_metric='euclidean', filemap=None):\n",
    "    canopies = dict()\n",
    "    X1_dist = pairwise_distances(X, metric=distance_metric)\n",
    "    #使用pairwise_distances函数（默认为'euclidean'欧几里得距离）来计算所有点之间的距离\n",
    "    canopy_points = set(range(X.shape[0]))\n",
    "    while canopy_points:\n",
    "        point = canopy_points.pop()\n",
    "        i = len(canopies)\n",
    "        canopies[i] = {\"c\":point, \"points\": list(np.where(X1_dist[point] < T2)[0])}\n",
    "        canopy_points = canopy_points.difference(set(np.where(X1_dist[point] < T1)[0]))\n",
    "    if filemap:\n",
    "        for canopy_id in canopies.keys():\n",
    "            canopy = canopies.pop(canopy_id)\n",
    "            canopy2 = {\"c\":filemap[canopy['c']], \"points\":list()}\n",
    "            for point in canopy['points']:\n",
    "                canopy2[\"points\"].append(filemap[point])\n",
    "            canopies[canopy_id] = canopy2\n",
    "    return canopies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于计算分类任务的几种性能指标，主要用于处理分类任务中的度量指标计算和聚类任务中的距离计算和误差计算，包含函数:Jaccard相似度、调整兰德系数、分类准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(y_true, y_pred):\n",
    "    y_true = y_true.map({\"D1\": 0, \"D2\": 1, \"D3\": 2, \"D4\": 3,\"b\":0, \"g\":1, 1: 0, 2: 1, 3: 2, \"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2})\n",
    "    \n",
    "    #print(y_true)\n",
    "    print(\"Jaccard: \", jaccard_score(y_true, y_pred, average=\"macro\"))    \n",
    "    print(\"AdjustedRandIndex: \",adjusted_rand_score(y_true, y_pred))\n",
    "    print(\"Accuracy: \",accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(vector1, vector2):\n",
    "        #print(vector1)\n",
    "        #print(vector2)\n",
    "        return np.sqrt(np.sum(np.power(vector1-vector2, 2)))\n",
    "    \n",
    "#计算一个中心点和一个样本点之间的欧几里得距离\n",
    "def getDistance(row_center, row_sample):\n",
    "        #print(row_center)\n",
    "        row_center = np.asarray(row_center)\n",
    "        #row_center = np.asarray(row_sample)\n",
    "        return euclideanDistance(row_center, row_sample)\n",
    "\n",
    "#计算K-Means聚类算法中每个簇的平方误差和\n",
    "def getSquaredError(data, kmeans, k):\n",
    "    distances = []\n",
    "    for i in range(k): # Qtd de clusters\n",
    "        distance = 0\n",
    "        for index_labels, value_labels in enumerate(kmeans.labels_): #kmeans.labels_ possui o cluster de cada elemento\n",
    "            if (i == value_labels):\n",
    "                #print(value_labels)\n",
    "                distance = distance + getDistance(kmeans.cluster_centers_[value_labels], data[index_labels])\n",
    "        distances.append(distance) #Erro quadratico medio de cada cluster\n",
    "    distances = np.asarray(distances)\n",
    "    error = np.sum(distances)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据读取与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"soybean-small\", \"iris\", \"wine\", \"segmentation\", \"ionosphere\"]\n",
    "ks = [4,3,3,7,2]\n",
    "kmeansTypes = [\"random\", \"k-means++\", \"densityCanopy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[4]\n",
    "k = ks[4]\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datasets/\"+file+\".data\", header=None)\n",
    "if file == \"segmentation\" or file == \"wine\": #Target eh na primeira coluna\n",
    "    target = data.iloc[:,0]\n",
    "    data = data.iloc[:,1:]\n",
    "else: #Target na última coluna\n",
    "    target = data.iloc[:,-1]\n",
    "    data = data.iloc[:,:-1]\n",
    "\n",
    "data = data.values\n",
    "distance_matrix = pairwise_distances(data, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义get_mean_distance函数用于计算距离矩阵中所有距离的平均值，如果数据集中只有一个样本点（即n=1），则直接返回距离矩阵的总和（在这种情况下，其实并没有真正的平均值可以计算，但函数仍然返回了一个值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition 1\n",
    "def get_mean_distance(distance_matrix):\n",
    "    n = len(distance_matrix)\n",
    "    return distance_matrix.sum() / (n* (n-1)) if n > 1 else distance_matrix.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义get_density函数用于计算每个样本点的“密度”（基于与平均距离的比较）。首先计算每个距离与平均距离之间的差值，并将所有大于或等于平均距离的值设为0，小于平均距离的值设为1。然后计算每一行中1的数量（即每个样本点与其他样本点之间的“密度”关系），并将这些值存储在densities数组中返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition 2\n",
    "def get_density(distance_matrix, mean_distance):\n",
    "    densities = np.zeros(len(distance_matrix))\n",
    "    \n",
    "    matrix = distance_matrix - mean_distance\n",
    "    matrix[matrix >= 0] = 0\n",
    "    matrix[matrix < 0] = 1\n",
    "\n",
    "    for i, row_i in enumerate(matrix):\n",
    "        densities[i] = row_i.sum()\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义cluster_tightness函数用于计算每个样本点所在簇的“紧密性”。紧密性可以理解为簇内样本点之间的平均接近程度。函数的具体步骤如下：  \n",
    "1.初始化一个与距离矩阵行数相同的零数组tightness。  \n",
    "2.遍历距离矩阵的每一行，代表每个样本点与其他样本点之间的距离。  \n",
    "3.对于每一行，筛选出所有小于平均距离的值，并计算它们的平均值（即簇内样本点之间的平均距离）。  \n",
    "4.如果这个平均值是0（即没有样本点与该样本点的距离小于平均距离），则紧密性为0；否则，紧密性设为1除以这个平均值。这个逆比例关系意味着，平均距离越小（即簇内样本点越紧密），紧密值越高。  \n",
    "5.返回包含所有样本点紧密性值的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition 3\n",
    "def cluster_tightness(distance_matrix, mean_distance):\n",
    "    tightness = np.zeros(len(distance_matrix))\n",
    "    \n",
    "    for i, row_i in enumerate(distance_matrix):\n",
    "        mean = row_i[row_i < mean_distance].mean()\n",
    "        tightness[i] = 0 if mean == 0 else 1 / mean\n",
    "    return tightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义clusters_dissimilarity函数用于计算每个簇与其他簇之间的不相似度。它基于两个假设：  \n",
    "1.一个簇内样本点之间的距离为0（在距离矩阵中表示0）。   \n",
    "2.簇之间的不相似度可以通过簇内样本点与其他簇的最小距离来衡量。   \n",
    "函数的具体步骤如下：   \n",
    "1.初始化一个与距离矩阵行数相同的零数组dissimilarity。  \n",
    "2.遍历距离矩阵的每一行，代表每个样本点与其他样本点之间的距离。  \n",
    "3.对于每一行，删除所有值为0的元素（即簇内样本点之间的距离）。   \n",
    "4.计算剩余距离中的最小值，作为该样本点所在簇与其他簇之间的不相似度。  \n",
    "5.找到密度最大的簇（即densities数组中的最大值对应的簇）。  \n",
    "6.将这个最大密度簇的不相似度设为它与其他簇之间的最大距离，可能是为了强调高密度簇与其他簇之间的最大差异。  \n",
    "7.返回包含所有簇的不相似度值的数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition 4\n",
    "def clusters_dissimilarity(densities, distance_matrix):\n",
    "    n = len(distance_matrix)\n",
    "    dissimilarity = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        d_row = distance_matrix[i]\n",
    "        d_row = np.delete(d_row, np.where(d_row == 0))\n",
    "        dissimilarity[i] = d_row.min()\n",
    "    max_id = np.where(densities == densities.max())\n",
    "    dissimilarity[max_id] = distance_matrix[max_id].max()\n",
    "    return dissimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_cluster函数用于移除以c为中心的簇的所有样本点。步骤为：      \n",
    "1.获取簇中心点的距离行：从距离矩阵中提取簇中心点 c 对应的行，存储在 center_row 中。   \n",
    "2.标记簇内样本点：将 center_row 中所有小于平均距离的值设为0。这些0值表示簇内的样本点（因为簇内样本点之间的距离通常很小）。   \n",
    "3.找出要移除的样本点：使用 np.where 函数找出 center_row 中所有值为0的位置，这些位置对应的样本点就是要从数据集中移除的簇内样本点。   \n",
    "4.移除样本点：使用 np.delete 函数从 data 数组中移除这些样本点。同时，也从距离矩阵 distance_matrix 中删除相应的行和列，以保持矩阵的维度一致性。   \n",
    "5.返回结果：返回更新后的数据和距离矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cluster(c, data, distance_matrix, mean_distance):\n",
    "    center_row = distance_matrix[c]\n",
    "    center_row[center_row < mean_distance] = 0\n",
    "    \n",
    "    #pontos a serem removidos\n",
    "    points = np.where(center_row == 0)\n",
    "    data = np.delete(data, points, axis=0)\n",
    "    \n",
    "    distance_matrix = np.delete(distance_matrix, points, axis=0)\n",
    "    distance_matrix = np.delete(distance_matrix, points, axis=1)\n",
    "    \n",
    "    return data, distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removeOutliers函数用于识别并移除数据集中的异常值。用于减少噪声数据对模型性能的影响。步骤为：      \n",
    "1.初始化异常值列表：创建一个空列表 outliers，用于存储识别出的异常值的索引。    \n",
    "2.遍历每个样本点：对于 aux_D 中的每一行（即每个样本点），检查其密度、逆紧密度和半径值。   \n",
    "3.识别异常值：如果某个样本点的密度等于1（可能是最孤立的点），并且其半径 s[i] 是数组 s 中的最大值，且数据集大小大于1（避免在只有一个样本点的情况下误判），则将该样本点视为异常值，并将其索引添加到 outliers 列表中。  \n",
    "4.移除异常值：使用 drop 方法从 aux_D 中移除识别出的异常值，并重置索引。同时，也从 densities、inv_a 和 s 数组中删除这些异常值对应的元素。  \n",
    "5.返回结果：返回更新后的 aux_D、densities、inv_a 和 s。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(aux_D, densities, inv_a, s, meanDis):\n",
    "    #remove elemento com densidade = 1 e que o s[i] seja maior que o raio\n",
    "    outliers = []\n",
    "    for i, row_i in enumerate(aux_D.values):\n",
    "        if densities[i] == 1 and s[i] == max(s) and aux_D.shape[0] > 1:\n",
    "            outliers.append(i)\n",
    "    aux_D.drop(outliers, inplace=True) #removendo outliers\n",
    "    aux_D.reset_index(drop=True, inplace=True)\n",
    "    densities = np.delete(densities, outliers, 0)\n",
    "    inv_a = np.delete(inv_a, outliers, 0)\n",
    "    s = np.delete(s, outliers, 0)\n",
    "    return aux_D, densities, inv_a, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义densityCanopy函数，它使用密度和紧密性概念来执行聚类分析，并最终返回聚类的中心点。    \n",
    "步骤一：  \n",
    "计算距离矩阵的平均距离，使用这个平均距离计算每个点的密度，找到密度最高的点，将其添加到中心点列表中，从数据和距离矩阵中移除该点所在的簇。  \n",
    "步骤二：   \n",
    "重新计算距离矩阵的平均距离，计算每个点的密度、紧密性和不相似度。通过将这三个值相乘，为每个点计算一个权重（w_set），找到权重最高的点，将其添加到中心点列表中，从数据和距离矩阵中移除该点所在的簇。   \n",
    "步骤三：\n",
    "进入一个循环，直到数据集中只剩下一个点或没有点为止。在每次循环中，重新计算距离矩阵的平均距离，计算每个点的密度和紧密性。计算当前数据点到已选中心点的距离（clusters_distance）。对于每个数据点，根据它到已选中心点的距离和它的密度、紧密性来计算权重。这个权重是该点到所有已选中心点的距离乘积与该点的密度和紧密性的乘积，找到权重最高的点，将其添加到中心点列表中。从数据和距离矩阵中移除该点所在的簇。   \n",
    "处理剩余点:如果数据集中只剩下一个点，将其添加到中心点列表中。  \n",
    "返回结果:返回包含所有聚类中心点的 centers 列表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的目的是通过结合密度、紧密性和到已选中心点的距离来识别聚类的中心点。每个步骤都旨在找到当前权重最高的点，将其添加到中心点列表中，并从数据集中移除其所在的簇。这种方法的优点是可以处理不同大小和密度的簇，并能够在不同簇之间找到平衡点。然而，该函数的性能可能受到数据大小和维度的影响，特别是在计算距离矩阵和权重时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densityCanopy():\n",
    "    # print (\"\\n----- \"+file+\" -----\")\n",
    "    centers = []\n",
    "    db = data.copy()\n",
    "    d_matrix = distance_matrix.copy()\n",
    "    \n",
    "    #Step 1\n",
    "    mean_distance = get_mean_distance(d_matrix)\n",
    "    density_set = get_density(d_matrix, mean_distance)\n",
    "    c = np.argmax(density_set)\n",
    "    centers.append(db[c])\n",
    "    db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "    \n",
    "    #Step 2\n",
    "    mean_distance = get_mean_distance(d_matrix)\n",
    "    density_set = get_density(d_matrix, mean_distance)\n",
    "    tightness_set = cluster_tightness(d_matrix, mean_distance)\n",
    "    dissimilarity_set = clusters_dissimilarity(density_set, d_matrix)\n",
    "    w_set = density_set * tightness_set * dissimilarity_set # Product Weight\n",
    "    c = np.argmax(w_set)\n",
    "    centers.append(db[c])\n",
    "    db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "    \n",
    "    #Step 3\n",
    "    while len(db) > 1:\n",
    "        \n",
    "        mean_distance = get_mean_distance(d_matrix)\n",
    "        density_set = get_density(d_matrix, mean_distance)\n",
    "        tightness_set = cluster_tightness(d_matrix, mean_distance)\n",
    "        \n",
    "        # Distancia das instancias para os centros já escolhidos\n",
    "        clusters_distance = pairwise_distances(db, centers, metric='euclidean')\n",
    "        \n",
    "        w_set = density_set * tightness_set\n",
    "        for i in range(len(db)):\n",
    "            w_set[i] = reduce(lambda x, y: x * y, clusters_distance[i] * w_set[i])\n",
    "        \n",
    "        c = np.argmax(w_set)\n",
    "        centers.append(db[c])\n",
    "        db, d_matrix = remove_cluster(c, db, d_matrix, mean_distance)\n",
    "        \n",
    "    if len(db) == 1:\n",
    "        centers.append(db[0])\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # demo_data = np.random.random_sample((2000, 5))\n",
    "    # print(demo_data.shape)\n",
    "    # data = auxData\n",
    "    # demo_data = data.values\n",
    "    clustering_time = []\n",
    "    clustering_error = []\n",
    "    print (\"\\n----- \"+file+\" -----\")\n",
    "    \n",
    "    # print(target)\n",
    "    #对kmeansType列表中的每一种方法执行一次聚类操作\n",
    "    for kmeansType in kmeansTypes:\n",
    "        print(kmeansType+\": \")\n",
    "        start = time()\n",
    "        if kmeansType == \"densityCanopy\":\n",
    "            centers = np.asarray(densityCanopy())\n",
    "            l = len(centers)\n",
    "            kmeans = KMeans(n_clusters=l, init=centers, n_init=1, max_iter=100).fit(data)\n",
    "        else:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=200, init=kmeansType, n_init=1, max_iter=100).fit(data)\n",
    "        \n",
    "    #计算聚类度量指标\n",
    "        getMetrics(target, kmeans.labels_)\n",
    "       #print(kmeans.labels_)\n",
    "    #计算错误率\n",
    "        error = getSquaredError(data, kmeans, k)\n",
    "        clustering_error.append(error)\n",
    "        print(\"Error:\", error,\"\\n\")\n",
    "\n",
    "        end = time()\n",
    "        clustering_time.append(end - start)\n",
    "        # print (\"\\n\")\n",
    "        \n",
    "    #输出每种聚类方法的运行时间和错误率\n",
    "    print (\"---Tempos---\")\n",
    "    print (\"random: \", clustering_time[0], \" kmeans++: \", clustering_time[1], \" densityCanopy: \", clustering_time[2])\n",
    "    \n",
    "    print (\"\\n---Erros---\")\n",
    "    print (\"random: \", clustering_error[0], \"kmeans++: \", clustering_error[1], \" densityCanopy: \", clustering_error[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- ionosphere -----\n",
      "random: \n",
      "Jaccard:  0.4765420899854862\n",
      "AdjustedRandIndex:  0.09040957289616172\n",
      "Accuracy:  0.6524216524216524\n",
      "Error: 2209.2071278041344 \n",
      "\n",
      "k-means++: \n",
      "Jaccard:  0.4707516339869281\n",
      "AdjustedRandIndex:  0.08646430021198863\n",
      "Accuracy:  0.6495726495726496\n",
      "Error: 2209.488950554777 \n",
      "\n",
      "densityCanopy: \n",
      "Jaccard:  0.08704459288680266\n",
      "AdjustedRandIndex:  0.04282060543347748\n",
      "Accuracy:  0.301994301994302\n",
      "Error: 1478.3214202267604 \n",
      "\n",
      "---Tempos---\n",
      "random:  0.03531837463378906  kmeans++:  0.038008689880371094  densityCanopy:  0.046010494232177734\n",
      "\n",
      "---Erros---\n",
      "random:  2209.2071278041344 kmeans++:  2209.488950554777  densityCanopy:  1478.3214202267604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码运行为对聚类算法的性能评估：具体是Jaccard相似度、调整兰德指数（Adjusted Rand Index）和准确率（Accuracy）的度量值，以及运行时间（Tempos）和错误率（Erros）的比较。\n",
    "\n",
    "聚类性能分析\n",
    "Jaccard相似度：用于比较两个样本集合的相似度。它的值介于0到1之间，值越大表示相似度越高。从结果看，第一个数据集的Jaccard相似度是0.5439，而ionosphere数据集的Jaccard相似度只有0.0333，这意味着在ionosphere数据集上，聚类的结果与实际类别之间的重叠很少，聚类效果可能不太理想。\n",
    "\n",
    "调整兰德指数（Adjusted Rand Index）：衡量聚类算法与真实分类之间相似性，取值范围为[-1,1]，值越高表示聚类结果越相似。在第一个数据集上，ARI为0.1776，而在ionosphere数据集上为0.2642。这两个值都相对较低，说明聚类结果与真实标签的匹配程度不高。\n",
    "\n",
    "准确率（Accuracy）：表示聚类算法正确分类样本的占比。第一个数据集的准确率为0.7122，而ionosphere数据集的准确率仅为0.1339。这进一步证实了ionosphere数据集的聚类效果较差。\n",
    "\n",
    "算法性能分析\n",
    "在Tempos部分，我们看到了三种方法（可能是random初始化、kmeans++初始化和densityCanopy算法）的运行时间。从时间上看，densityCanopy的运行时间介于random初始化和kmeans++之间。但是，运行时间不是评价聚类效果的唯一标准，还需要结合聚类质量来综合判断。\n",
    "\n",
    "错误率分析\n",
    "在Erros部分，提供了三种方法的错误率。错误率通常是通过计算聚类结果与实际标签之间的差异来得到的。从结果看，densityCanopy的错误率最低（510.9547），而random初始化和kmeans++的错误率相同且最高（796.4667）。这表明densityCanopy在这个任务上可能具有更好的性能。\n",
    "\n",
    "总结\n",
    "在ionosphere数据集上，所有方法的聚类效果都不是很理想，可能需要进一步调整参数或尝试其他聚类算法。\n",
    "densityCanopy算法在错误率方面表现最好，但在调整兰德指数和准确率方面仍有提升空间。\n",
    "运行时间是一个考虑因素，但在实际应用中，通常更关注聚类效果。\n",
    "为了改进聚类效果，可以考虑以下方法：\n",
    "\n",
    "调整聚类算法的参数。\n",
    "尝试不同的聚类算法。\n",
    "对数据进行预处理，例如降维或标准化，以改善聚类效果。\n",
    "深入分析数据特性，了解数据的分布和结构，以便选择合适的聚类策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"soybean-small\", \"iris\", \"wine\", \"segmentation\", \"ionosphere\"]\n",
    "ks = [4,3,3,7,2]\n",
    "kmeansTypes = [\"random\", \"k-means++\", \"densityCanopy\"]\n",
    "\n",
    "file = files[4]\n",
    "k = ks[4]\n",
    "\n",
    "mu, sigma = 5, 1\n",
    "\n",
    "#display(data)\n",
    "data = pd.read_csv(\"datasets/\"+file+\".data\", header=None)\n",
    "if file == \"segmentation\" or file == \"wine\": #Target eh na primeira coluna\n",
    "    target = data.iloc[:,0]\n",
    "    data = data.iloc[:,1:]\n",
    "else: #Target na última coluna\n",
    "    target = data.iloc[:,-1]\n",
    "    data = data.iloc[:,:-1]\n",
    "\n",
    "noise = np.random.normal(mu, sigma, [data.shape[0], data.shape[1]])\n",
    "\n",
    "data = data + noise\n",
    "#display(data)\n",
    "data = data.values\n",
    "distance_matrix = pairwise_distances(data, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码首先定义了一个包含数据集名称的列表files、一个包含聚类数（k值）的列表ks以及一个包含聚类方法类型的列表kmeansTypes。接下来，它选择files列表中的最后一个数据集ionosphere和对应的k值2进行后续操作。\n",
    "\n",
    "代码接着读取指定数据集的文件，并根据数据集的特点（是segmentation或wine数据集还是其他数据集）确定目标变量（标签）的位置，并将其从数据中分离出来。之后，代码向数据中添加正态分布的噪声。\n",
    "\n",
    "随后，代码将数据转换为NumPy数组，并计算数据点之间的欧几里得距离矩阵。\n",
    "\n",
    "接下来，调用run()函数执行聚类分析，并输出Jaccard相似度、调整兰德指数和准确率作为聚类效果的度量。从输出结果来看，ionosphere数据集的聚类效果相对较差，Jaccard相似度、调整兰德指数和准确率都较低。\n",
    "\n",
    "在“Tempos”部分，代码输出了三种聚类方法（random初始化、kmeans++初始化和densityCanopy算法）的运行时间。从输出结果来看，densityCanopy算法的运行时间最短，而random初始化方法的运行时间最长。\n",
    "\n",
    "在“Erros”部分，代码输出了三种方法的错误率。错误率是通过计算聚类结果与实际标签之间的差异来得到的。从结果看，densityCanopy算法的错误率最低，而random初始化和kmeans++的错误率相对较高。\n",
    "\n",
    "与上一段代码的执行效果相比，这段代码选择了不同的数据集（ionosphere）和聚类数（k=2），并可能使用了相同的聚类算法和参数设置。从结果来看，两段代码在聚类效果上存在差异，这可能是由于数据集的不同导致的。此外，虽然densityCanopy算法在错误率方面表现较好，但在调整兰德指数和准确率方面仍有提升空间。\n",
    "\n",
    "总结来说，这段代码通过读取数据集、添加噪声、计算距离矩阵和执行聚类分析，评估了不同聚类方法在特定数据集上的性能。与上一段代码相比，尽管聚类算法可能相同，但由于数据集和参数的不同，聚类效果存在差异。为了进一步提高聚类效果，可以尝试调整聚类算法的参数、尝试不同的聚类算法或对数据进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard:  0.4765420899854862\n",
      "AdjustedRandIndex:  0.09040957289616172\n",
      "Accuracy:  0.6524216524216524\n",
      "Jaccard:  0.4707516339869281\n",
      "AdjustedRandIndex:  0.08646430021198863\n",
      "Accuracy:  0.6495726495726496\n",
      "\n",
      "----- ionosphere -----\n",
      "Jaccard:  0.08704459288680266\n",
      "AdjustedRandIndex:  0.04282060543347748\n",
      "Accuracy:  0.301994301994302\n",
      "\n",
      "---Tempos---\n",
      "random:  0.031957149505615234  kmeans++:  0.0400090217590332  densityCanopy:  0.04301023483276367\n",
      "\n",
      "---Erros---\n",
      "random:  2209.2071278041344 kmeans++:  2209.488950554777  densityCanopy:  1478.3214202267604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1334: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
